V_LORA_R=(8 16 32)
V_LORA_ALPHA=(16 32 64)
V_LORA_DROPOUT=(0.1 0.2 0.3)
V_NUM_TRAIN_EPOCHS=(2 3 4)
V_MAX_GRAD_NORM=(1.0 1.0 1.0)
V_LEARNING_RATE=(2e-5 2e-4 3e-4)
V_WEIGHT_DECAY=(.01 .005 .001)
V_WARMUP_RATIO=(0.1 0.2 0.3)
V_MAX_SEQ_LENGTH=(1024 2048 8192)
# Set values to override trigger
#HP_IMAGE_URL=us-docker.pkg.dev/gkebatchexpce3c8dcb/llm/finetune:llama3
#HP_EXPERIMENT=llama3
#HP_MODEL_NAME=meta-llama/Meta-Llama-3-70B
#HP_MODEL_BUCKET=kr-finetune
#HP_MODEL_PATH=model-data/llama3-a100/experiment
#HP_TRAINING_DATASET_BUCKET=kh-finetune-ds
#HP_TRAINING_DATASET_PATH=dataset/output-abc12311